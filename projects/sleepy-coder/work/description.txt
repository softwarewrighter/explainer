Towards Continuous LLM Learning (Part 1): The Averaging Trap

What if your coding assistant could learn from its mistakes overnight? This is our vision for continuous learning in LLMs.

The challenge: catastrophic forgetting. When you train a model on new patterns, it forgets what it already knew. Fix one bug, break three others.

We implemented the Share algorithm from recent Johns Hopkins research. It extracts a shared subspace using SVD, allowing new tasks to train with 67x fewer parameters. The Sleepy Coder system runs a day/night cycle - the agent fixes Rust compiler errors during the day, and we train new adapters overnight.

The results: 76.7% accuracy with task-specific coefficients, and zero regressions when using the full Share algorithm.

The key insight: routing beats averaging. When we averaged coefficients across tasks, the gains disappeared. Specialists outperform generalists.

Part 2 will implement task routing - detecting error types and selecting the matching adapter.

---

GitHub (Sleepy Coder): https://github.com/softwarewrighter/sleepy-coder

Research Papers:
- Share Algorithm: https://arxiv.org/abs/2602.06043
- UWSH (Universal Weight Subspace Hypothesis): https://arxiv.org/abs/2512.05117

---

#MachineLearning #LLM #ContinualLearning #RustLang #AI #DeepLearning #NeuralNetworks
